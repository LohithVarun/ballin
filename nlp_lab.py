# -*- coding: utf-8 -*-
"""NLP LAB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sp_ySA1Ix2cy_uQKbZDsm2frpxYNE9TZ

1. Program
"""

import re

text = "The rain in Spain falls mainly in the plain."

# Pattern to match words ending with 'ain'
pattern = r'\b\w*ain\b'

# Searching for pattern
match = re.search(pattern, text)
if match:
    print("First Match:", match.group())

# Finding all matches
matches = re.findall(pattern, text)
print("All Matches:", matches)

# Replacing matches
new_text = re.sub(pattern, '***', text)
print("Modified Text:", new_text)

"""2. Program"""

def fsa_match_ab(string):
    state = 0  # Initial state

    for char in string:
        if state == 0 and char == 'a':
            state = 1
        elif state == 1 and char == 'b':
            state = 2
        elif char == 'a':  # Reset to state 1 if 'a' appears
            state = 1
        else:
            state = 0  # Reset state

    return state == 2  # Accept if in final state

# Test cases
strings = ["ab", "cab", "aaab", "baba", "xyzab", "abab"]
for s in strings:
    print(f"'{s}' ->", "Accepted" if fsa_match_ab(s) else "Rejected")

"""3. program"""

import nltk
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

words = ["running", "flies", "better", "cats", "geese"]

for word in words:
    print(f"{word} -> {lemmatizer.lemmatize(word, pos='v')}")  # Verb lemmatization
    print(f"{word} -> {lemmatizer.lemmatize(word, pos='n')}")  # Noun lemmatization

"""4. Program"""

def pluralize(noun):
    if noun.endswith("y") and noun[-2] not in "aeiou":
        return noun[:-1] + "ies"
    elif noun.endswith(("s", "x", "z", "sh", "ch")):
        return noun + "es"
    elif noun.endswith("f"):
        return noun[:-1] + "ves"
    elif noun.endswith("fe"):
        return noun[:-2] + "ves"
    else:
        return noun + "s"

# Test cases
nouns = ["cat", "dog", "baby", "fox", "leaf", "knife"]
for noun in nouns:
    print(f"{noun} -> {pluralize(noun)}")

"""5.Program"""

from nltk.stem import PorterStemmer

# Initialize the Porter Stemmer
stemmer = PorterStemmer()

# List of words to be stemmed
words = ["running", "runner", "easily", "fairness", "happiness", "flying", "caresses"]

# Perform stemming on each word and print the result
for word in words:
    stemmed_word = stemmer.stem(word)
    print(f"Original Word: {word} -> Stemmed Word: {stemmed_word}")

"""6.Program"""

# prompt: Implement a basic N-gram model for text generation. For example, generate text using a
# bigram model using python. check out the ouput correctly

import re
from collections import defaultdict

def generate_bigram_text(text, length=20):
    # Preprocess the text
    text = re.sub(r'[^\w\s]', '', text).lower()  # Remove punctuation and lowercase
    words = text.split()

    # Build the bigram model
    bigram_model = defaultdict(list)
    for i in range(len(words) - 1):
        bigram_model[words[i]].append(words[i+1])

    # Generate text
    current_word = words[0]  # Start with the first word
    generated_text = [current_word]
    for _ in range(length - 1):
      if current_word in bigram_model:
        next_word = bigram_model[current_word][0] # take the first word from the bigram model
        generated_text.append(next_word)
        current_word = next_word
      else:
        # Handle cases where current word is not in the model (e.g., end of sentence)
        generated_text.append(words[0])
        current_word = words[0]

    return " ".join(generated_text)

# Example usage
text = "The rain in Spain falls mainly in the plain. The sun shines brightly."
generated_text = generate_bigram_text(text)
generated_text

import nltk
nltk.data.path

import nltk
nltk.download('all')

"""7.program"""

import nltk
from nltk import word_tokenize, pos_tag

# Ensure necessary resources are downloaded
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Sample text
text = "Natural Language Processing is fascinating. It helps computers understand human language."

# Tokenize the text into words
tokens = word_tokenize(text)

# Perform POS tagging
tagged_words = pos_tag(tokens)

# Print the tagged words
print("Part-of-Speech Tagged Words:")
for word, tag in tagged_words:
    print(f"{word} -> {tag}")

"""8."""

import nltk
from nltk.corpus import treebank
from nltk import HiddenMarkovModelTagger

nltk.download('treebank')

# Load dataset
train_data = treebank.tagged_sents()[:3000]  # Use the first 3000 sentences for training
test_data = treebank.tagged_sents()[3000:]  # Remaining for testing

# Train HMM Tagger
hmm_tagger = HiddenMarkovModelTagger.train(train_data)

# Test the tagger
sentence = "Ram is a good  boy.".split()
tagged_sentence = hmm_tagger.tag(sentence)

print("Tagged Sentence:", tagged_sentence)

"""9."""

import nltk
from nltk import RegexpTagger

patterns = [
    (r'.*ing$', 'VBG'),     # Gerunds (e.g., 'playing')
    (r'.*ed$', 'VBD'),      # Past tense verbs (e.g., 'played')
    (r'.*es$', 'VBZ'),      # Present tense verbs (e.g., 'writes')
    (r'.*\'s$', 'POS'),     # Possessives (e.g., "John's")
    (r'\b\w+\b', 'NN')      # Default noun for unknown words
]

tagger = RegexpTagger(patterns)

sentence = "The cat is playing with a ball."
tagged_sentence = tagger.tag(sentence.split())

print("Tagged Sentence:", tagged_sentence)

"""10."""

import nltk
from nltk.corpus import treebank
from nltk.tag import brill, brill_trainer

nltk.download('treebank')
train_data = treebank.tagged_sents()[:300]

# Create a simple transformation rule
templates = brill.nltkdemo18()  # Use pre-defined templates

trainer = brill_trainer.BrillTaggerTrainer(nltk.DefaultTagger('NN'), templates)
brill_tagger = trainer.train(train_data, max_rules=50)

# Test the tagger
sentence = "The dog barked loudly.".split()
tagged_sentence = brill_tagger.tag(sentence)

print("Tagged Sentence:", tagged_sentence)

"""11."""

import nltk
from nltk import CFG


grammar = CFG.fromstring("""
    S -> NP VP
    NP -> Det N | Det N PP
    VP -> V NP | V NP PP
    PP -> P NP
    Det -> 'the' | 'a'
    N -> 'dog' | 'cat' | 'park' | 'telescope'
    V -> 'saw' | 'walked'
    P -> 'in' | 'with'
""")

sentence = ['the', 'dog', 'saw', 'a', 'cat', 'in', 'the', 'park']

parser = nltk.TopDownChartParser(grammar)
for tree in parser.parse(sentence):
    print(tree)

"""12."""

import nltk
from nltk import CFG

# Define a simple context-free grammar
grammar = CFG.fromstring("""
    S -> NP VP
    NP -> Det N | Det N PP
    VP -> V NP | V NP PP
    PP -> P NP
    Det -> 'the' | 'a'
    N -> 'dog' | 'cat' | 'park' | 'telescope'
    V -> 'saw' | 'walked'
    P -> 'in' | 'with'
""")

sentence = ['the', 'dog', 'saw', 'a', 'cat', 'in', 'the', 'park']

parser = nltk.EarleyChartParser(grammar)
for tree in parser.parse(sentence):
    print(tree)

"""13. Generate a Parse Tree Using Context-Free Grammar (CFG)"""

import nltk
from nltk import CFG

# Define a simple context-free grammar
grammar = CFG.fromstring("""
    S -> NP VP
    NP -> Det N | Det N PP
    VP -> V NP | V NP PP
    PP -> P NP
    Det -> 'the' | 'a'
    N -> 'dog' | 'cat' | 'park' | 'telescope'
    V -> 'saw' | 'walked'
    P -> 'in' | 'with'
""")

sentence = ['the', 'dog', 'saw', 'a', 'cat']
parser = nltk.ChartParser(grammar)

# Display parse trees as text
for tree in parser.parse(sentence):
    print(tree)

"""14.Check Agreement in Sentences Based on CFG Rules"""

import nltk
from nltk import CFG

# Define a grammar with agreement rules for subject-verb agreement
grammar = CFG.fromstring("""
    S -> NP VP
    NP -> Det N
    VP -> V
    Det -> 'the'
    N -> 'dog' | 'dogs'
    V -> 'barks' | 'bark'
""")

def check_agreement(sentence):
    parser = nltk.ChartParser(grammar)
    for tree in parser.parse(sentence):
        return "Agreement found: " + str(tree)
    return "No agreement found."

sentence1 = ['the', 'dog', 'barks']  # Correct
sentence2 = ['the', 'dog', 'bark']   # Incorrect

print("Sentence 1:", check_agreement(sentence1))
print("Sentence 2:", check_agreement(sentence2))

"""15. Probabilistic Context-Free Grammar Parsing"""

import nltk
from nltk import PCFG

# Define a probabilistic CFG
grammar = PCFG.fromstring("""
    S -> NP VP [1.0]
    NP -> Det N [0.8] | Det N PP [0.2]
    VP -> V NP [0.6] | V NP PP [0.4]
    PP -> P NP [1.0]
    Det -> 'the' [0.6] | 'a' [0.4]
    N -> 'dog' [0.5] | 'cat' [0.5]
    V -> 'saw' [1.0]
    P -> 'with' [1.0]
""")

sentence = ['the', 'dog', 'saw', 'a', 'cat']
parser = nltk.ViterbiParser(grammar)

# Parse the sentence
for tree in parser.parse(sentence):
    print(tree)

"""16.Named Entity Recognition (NER) Using SpaCy"""

import spacy

# Load SpaCy's English language model
nlp = spacy.load('en_core_web_sm')

# Sample text
text = "Barack Obama was born in Hawaii. He was elected president of the United States in 2008."

# Process the text
doc = nlp(text)

# Print named entities
print("Named Entities:")
for ent in doc.ents:
    print(f"{ent.text} -> {ent.label_}")

"""17.Access WordNet for Synsets and Word Meanings"""

from nltk.corpus import wordnet as wn
import nltk
nltk.download('wordnet')

# Get synsets for a word
word = "dog"
synsets = wn.synsets(word)

print(f"Synsets for '{word}':")
for synset in synsets:
    print(f"{synset.name()} - {synset.definition()}")

"""18.First-Order Predicate Calculus (FOPC) Parser"""

class FOPCParser:
    def parse(self, expression):
        tokens = expression.replace('(', ' ( ').replace(')', ' ) ').split()
        return self._parse_tokens(tokens)

    def _parse_tokens(self, tokens):
        if len(tokens) == 0:
            raise SyntaxError("Unexpected end of expression")
        token = tokens.pop(0)
        if token == '(':
            sub_expression = []
            while tokens[0] != ')':
                sub_expression.append(self._parse_tokens(tokens))
            tokens.pop(0)  # Remove ')'
            return sub_expression
        else:
            return token

parser = FOPCParser()
expression = "(forall x (implies (P x) (Q x)))"
parsed_expression = parser.parse(expression)
print("Parsed Expression:", parsed_expression)

"""19.Word Sense Disambiguation Using the Lesk Algorithm"""

from nltk.wsd import lesk
from nltk.tokenize import word_tokenize
import nltk
nltk.download('wordnet')
nltk.download('punkt')

# Sample sentence and ambiguous word
sentence = "I went to the bank to deposit money."
ambiguous_word = "bank"

# Tokenize the sentence
tokens = word_tokenize(sentence)

# Perform Lesk algorithm for word sense disambiguation
sense = lesk(tokens, ambiguous_word)
print(f"Disambiguated Sense for '{ambiguous_word}': {sense.name()} - {sense.definition()}")

"""20. Basic Information Retrieval System Using TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Sample documents
documents = [
    "Natural Language Processing is a fascinating field.",
    "Information Retrieval is an application of Natural Language Processing.",
    "I love learning about machine learning and data science."
]

# Compute TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Display TF-IDF scores
feature_names = vectorizer.get_feature_names_out()
for doc_idx, doc in enumerate(tfidf_matrix.toarray()):
    print(f"Document {doc_idx + 1}:")
    for word_idx, score in enumerate(doc):
        if score > 0:
            print(f"  {feature_names[word_idx]}: {score:.4f}")